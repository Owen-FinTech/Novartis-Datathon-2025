{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Helper file to locally compute Datathon 2025 Metrics.\n",
    "This file is intended to be used by participants to test the metrics\n",
    "using custom train/validation splits and also to generate submission files.\n",
    "\n",
    "Metrics supported:\n",
    "- Metric 1 (Phase 1-a): 0 actuals\n",
    "- Metric 2 (Phase 1-b): 6 actuals\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Metric 1 (Phase 1-a)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def _compute_pe_phase1a(group: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute PE for one (country, brand, bucket) group following the corrected Metric 1 formula.\"\"\"\n",
    "    avg_vol = group[\"avg_vol\"].iloc[0]\n",
    "    if avg_vol == 0 or np.isnan(avg_vol):\n",
    "        return np.nan\n",
    "\n",
    "    def sum_abs_diff(month_start: int, month_end: int) -> float:\n",
    "        \"\"\"Sum of absolute differences sum(|actual - pred|).\"\"\"\n",
    "        subset = group[(group[\"months_postgx\"] >= month_start) & (group[\"months_postgx\"] <= month_end)]\n",
    "        return (subset[\"volume_actual\"] - subset[\"volume_predict\"]).abs().sum()\n",
    "    \n",
    "    def abs_sum_diff(month_start: int, month_end: int) -> float:\n",
    "        \"\"\"Absolute difference of |sum(actuals) - sum(pred)|.\"\"\"\n",
    "        subset = group[(group[\"months_postgx\"] >= month_start) & (group[\"months_postgx\"] <= month_end)]\n",
    "        sum_actual = subset[\"volume_actual\"].sum()\n",
    "        sum_pred = subset[\"volume_predict\"].sum()\n",
    "        return abs(sum_actual - sum_pred)\n",
    "\n",
    "    term1 = 0.2 * sum_abs_diff(0, 23) / (24 * avg_vol)\n",
    "    term2 = 0.5 * abs_sum_diff(0, 5) / (6 * avg_vol)\n",
    "    term3 = 0.2 * abs_sum_diff(6, 11) / (6 * avg_vol)\n",
    "    term4 = 0.1 * abs_sum_diff(12, 23) / (12 * avg_vol)\n",
    "\n",
    "    return term1 + term2 + term3 + term4\n",
    "\n",
    "\n",
    "def _metric1(df_actual: pd.DataFrame, df_pred: pd.DataFrame, df_aux: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute Metric 1 PE value.\n",
    "\n",
    "    :param df_actual: Actual volume data\n",
    "    :param df_pred: Predicted volume data\n",
    "    :param df_aux: Auxiliary data with buckets and avg_vol\n",
    "    :return: Weighted PE total (Phase 1)\n",
    "    \"\"\"\n",
    "    merged = df_actual.merge(\n",
    "        df_pred,\n",
    "        on=[\"country\", \"brand_name\", \"months_postgx\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_actual\", \"_predict\")\n",
    "    ).merge(df_aux, on=[\"country\", \"brand_name\"], how=\"left\")\n",
    "\n",
    "    merged[\"start_month\"] = merged.groupby([\"country\", \"brand_name\"])[\"months_postgx\"].transform(\"min\")\n",
    "    merged = merged[merged[\"start_month\"] == 0].copy()\n",
    "\n",
    "    pe_results = (\n",
    "        merged.groupby([\"country\", \"brand_name\", \"bucket\"])\n",
    "        .apply(_compute_pe_phase1a)\n",
    "        .reset_index(name=\"PE\")\n",
    "    )\n",
    "\n",
    "    bucket1 = pe_results[pe_results[\"bucket\"] == 1]\n",
    "    bucket2 = pe_results[pe_results[\"bucket\"] == 2]\n",
    "\n",
    "    n1 = bucket1[[\"country\", \"brand_name\"]].drop_duplicates().shape[0]\n",
    "    n2 = bucket2[[\"country\", \"brand_name\"]].drop_duplicates().shape[0]\n",
    "\n",
    "    return (2/n1) * bucket1[\"PE\"].sum() + (1/n2) * bucket2[\"PE\"].sum()\n",
    "\n",
    "\n",
    "def compute_metric1(\n",
    "    df_actual: pd.DataFrame,\n",
    "    df_pred: pd.DataFrame,\n",
    "    df_aux: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute Metric 1 (Phase 1).\n",
    "\n",
    "    :param df_actual: Actual volume data\n",
    "    :param df_pred: Predicted volume data\n",
    "    :param df_aux: Auxiliary data with buckets and avg_vol\n",
    "    :return: Computed Metric 1 value\n",
    "    \"\"\"\n",
    "    return round(_metric1(df_actual, df_pred, df_aux), 4)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Metric 2 (Phase 1-b)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def _compute_pe_phase1b(group: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute PE for a specific country-brand-bucket group.\n",
    "\n",
    "    :param group: DataFrame group with abs_diff and avg_vol columns\n",
    "    :return: PE value for the group\n",
    "    \"\"\"\n",
    "    avg_vol = group[\"avg_vol\"].iloc[0]\n",
    "    if avg_vol == 0 or np.isnan(avg_vol):\n",
    "        return np.nan\n",
    "\n",
    "    def sum_abs_diff(month_start: int, month_end: int) -> float:\n",
    "        \"\"\"Sum of absolute differences sum(|actual - pred|).\"\"\"\n",
    "        subset = group[(group[\"months_postgx\"] >= month_start) & (group[\"months_postgx\"] <= month_end)]\n",
    "        return (subset[\"volume_actual\"] - subset[\"volume_predict\"]).abs().sum()\n",
    "    \n",
    "    def abs_sum_diff(month_start: int, month_end: int) -> float:\n",
    "        \"\"\"Absolute difference of |sum(actuals) - sum(pred)|.\"\"\"\n",
    "        subset = group[(group[\"months_postgx\"] >= month_start) & (group[\"months_postgx\"] <= month_end)]\n",
    "        sum_actual = subset[\"volume_actual\"].sum()\n",
    "        sum_pred = subset[\"volume_predict\"].sum()\n",
    "        return abs(sum_actual - sum_pred)\n",
    "\n",
    "    term1 = 0.2 * sum_abs_diff(6, 23) / (18 * avg_vol)\n",
    "    term2 = 0.5 * abs_sum_diff(6, 11) / (6 * avg_vol)\n",
    "    term3 = 0.3 * abs_sum_diff(12, 23) / (12 * avg_vol)\n",
    "    \n",
    "    return term1 + term2 + term3\n",
    "\n",
    "\n",
    "def _metric2(df_actual: pd.DataFrame, df_pred: pd.DataFrame, df_aux: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute Metric 2 PE value.\n",
    "\n",
    "    :param df_actual: Actual volume data\n",
    "    :param df_pred: Predicted volume data\n",
    "    :param df_aux: Auxiliary data with buckets and avg_vol\n",
    "    :return: Weighted PE total (Phase 2)\n",
    "    \"\"\"\n",
    "    merged_data = df_actual.merge(\n",
    "        df_pred,\n",
    "        on=[\"country\", \"brand_name\", \"months_postgx\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_actual\", \"_predict\")\n",
    "    ).merge(df_aux, on=[\"country\", \"brand_name\"], how=\"left\")\n",
    "\n",
    "    merged_data[\"start_month\"] = merged_data.groupby([\"country\", \"brand_name\"])[\"months_postgx\"].transform(\"min\")\n",
    "    merged_data = merged_data[merged_data[\"start_month\"] == 6].copy()\n",
    "\n",
    "    pe_results = (\n",
    "        merged_data.groupby([\"country\", \"brand_name\", \"bucket\"])\n",
    "        .apply(_compute_pe_phase1b)\n",
    "        .reset_index(name=\"PE\")\n",
    "    )\n",
    "\n",
    "    bucket1 = pe_results[pe_results[\"bucket\"] == 1]\n",
    "    bucket2 = pe_results[pe_results[\"bucket\"] == 2]\n",
    "\n",
    "    n1 = bucket1[[\"country\", \"brand_name\"]].drop_duplicates().shape[0]\n",
    "    n2 = bucket2[[\"country\", \"brand_name\"]].drop_duplicates().shape[0]\n",
    "    \n",
    "    return (2/n1) * bucket1[\"PE\"].sum() + (1/n2) * bucket2[\"PE\"].sum()\n",
    "\n",
    "\n",
    "def compute_metric2(\n",
    "    df_actual: pd.DataFrame,\n",
    "    df_pred: pd.DataFrame,\n",
    "    df_aux: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute Metric 2 (Phase 2).\n",
    "\n",
    "    :param df_actual: Actual volume data\n",
    "    :param df_pred: Predicted volume data\n",
    "    :param df_aux: Auxiliary data with buckets and avg_vol\n",
    "    :return: Computed Metric 2 value\n",
    "    \"\"\"\n",
    "    return round(_metric2(df_actual, df_pred, df_aux), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33081fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_train_full = pd.read_csv(\"../data/processed/df_train_merged_filled.csv\")\n",
    "df_test_full = pd.read_csv(\"../data/processed/df_test_merged_filled.csv\")\n",
    "\n",
    "# One-hot-encoding categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "encoded = encoder.fit_transform(df_train_full[[\"month\", \"ther_area\", \"main_package\", \"biological\", \"small_molecule\", \"bucket\"]])\n",
    "encoder.get_feature_names_out([\"month\", \"ther_area\", \"main_package\", \"biological\", \"small_molecule\", \"bucket\"])\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(), index=df_train_full.index)\n",
    "df_train_full = df_train_full.drop([\"month\", \"ther_area\", \"main_package\", \"biological\", \"small_molecule\"], axis=1).join(encoded_df)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "encoded = encoder.fit_transform(df_test_full[[\"month\", \"ther_area\", \"main_package\", \"biological\", \"small_molecule\", \"bucket\"]])\n",
    "encoder.get_feature_names_out([\"month\", \"ther_area\", \"main_package\", \"biological\", \"small_molecule\", \"bucket\"])\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(), index=df_test_full.index)\n",
    "df_test_full = df_test_full.drop([\"month\", \"ther_area\", \"main_package\", \"biological\", \"small_molecule\"], axis=1).join(encoded_df)\n",
    "\n",
    "# Manually adding missing one-hot column\n",
    "df_test_full[\"ther_area_Systemic_Hormones\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train + test with a flag\n",
    "df_train_full = df_train_full.copy()\n",
    "df_test_full = df_test_full.copy()\n",
    "\n",
    "df_train_full[\"is_train\"] = 1\n",
    "df_test_full[\"is_train\"] = 0\n",
    "\n",
    "df_all = pd.concat([df_train_full, df_test_full], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98fce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by group + time\n",
    "time_cols = [\"country\", \"brand_name\", \"months_postgx\"]\n",
    "df_all = df_all.sort_values(time_cols)\n",
    "\n",
    "group = df_all.groupby([\"country\", \"brand_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e5af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag features\n",
    "df_all[\"lag_1\"] = group[\"volume\"].shift(1)\n",
    "df_all[\"lag_2\"] = group[\"volume\"].shift(2)\n",
    "df_all[\"lag_3\"] = group[\"volume\"].shift(3)\n",
    "\n",
    "# Last year's value (12 months ago in same group)\n",
    "df_all[\"last_year_volume\"] = group[\"volume\"].shift(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling averages (past-only, per group)\n",
    "\n",
    "# Helper: rolling mean of *past* values (shifted by 1)\n",
    "roll3 = group[\"volume\"].apply(\n",
    "    lambda s: s.shift(1).rolling(window=3, min_periods=1).mean()\n",
    ")\n",
    "roll6 = group[\"volume\"].apply(\n",
    "    lambda s: s.shift(1).rolling(window=6, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# roll3/roll6 now have a MultiIndex (country, brand_name, original_index)\n",
    "# Drop the first two levels so index lines up with df_all\n",
    "df_all[\"roll3_past\"] = roll3.reset_index(level=[0, 1], drop=True)\n",
    "df_all[\"roll6_past\"] = roll6.reset_index(level=[0, 1], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e0baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group-level mean\n",
    "df_all[\"group_volume_mean\"] = group[\"volume\"].transform(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs in lag/rolling features\n",
    "lag_cols = [\n",
    "    \"lag_1\", \"lag_2\", \"lag_3\",\n",
    "    \"roll3_past\", \"roll6_past\",\n",
    "    \"last_year_volume\",\n",
    "]\n",
    "\n",
    "df_all[lag_cols] = df_all[lag_cols].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81450d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split back into train and test\n",
    "df_train_full = df_all[df_all[\"is_train\"] == 1].drop(columns=[\"is_train\"])\n",
    "df_test_full  = df_all[df_all[\"is_train\"] == 0].drop(columns=[\"is_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df01629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .csv files\n",
    "df_train_full.to_csv(\"../data/processed/df_train_unscaled.csv\", index=False)\n",
    "df_test_full.to_csv(\"../data/processed/df_test_unscaled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"months_postgx\",\n",
    "    # \"n_gxs\",\n",
    "    # \"hospital_rate\",\n",
    "    # \"mean_generic_erosion\"\n",
    "    \"lag_1\", \n",
    "    # \"lag_2\", \n",
    "    # \"lag_3\", \n",
    "    \"roll3_past\", \n",
    "    \"roll6_past\", \n",
    "    \"group_volume_mean\" \n",
    "    # \"last_year_volume\"\n",
    "]\n",
    "\n",
    "categorical_onehot_features = [\n",
    "    # ther_area\n",
    "    \"ther_area_Anti-infectives\",\n",
    "    # \"ther_area_Antineoplastic_and_immunology\",\n",
    "    # \"ther_area_Cardiovascular_Metabolic\",\n",
    "    # \"ther_area_Dermatology\",\n",
    "    \"ther_area_Endocrinology_and_Metabolic_Disease\",\n",
    "    # \"ther_area_Haematology\",\n",
    "    # \"ther_area_Muscoskeletal_Rheumatology_and_Osteology\",\n",
    "    # \"ther_area_Nervous_system\",\n",
    "    # \"ther_area_Obstetrics_Gynaecology\",\n",
    "    \"ther_area_Others\",\n",
    "    # \"ther_area_Parasitology\",\n",
    "    \"ther_area_Respiratory_and_Immuno-inflammatory\",\n",
    "    \"ther_area_Sensory_organs\",\n",
    "    # \"ther_area_Systemic_Hormones\", \n",
    "\n",
    "    # main_package\n",
    "    \"main_package_PATCH\"\n",
    "    # \"main_package_PILL\",\n",
    "\n",
    "    # biological / small molecule flags\n",
    "    # \"small_molecule_False\", \n",
    "    # \"small_molecule_True\"\n",
    "]\n",
    "\n",
    "extra_features = [\n",
    "    # main_package\n",
    "    # \"main_package_CREAM\",\n",
    "    # \"main_package_EYE DROP\",\n",
    "    \"main_package_INJECTION\",\n",
    "    # \"main_package_Others\",\n",
    "    \n",
    "    # biological / small molecule flags\n",
    "    \"biological_False\" \n",
    "    # \"biological_True\"\n",
    "    \n",
    "    # Only for split to create df_aux\n",
    "    # \"country\", \n",
    "    # \"brand_name\", \n",
    "    # \"avg_vol\", \n",
    "    # \"bucket\"\n",
    "    # \"bucket_1\", \n",
    "    # \"bucket_2\"\n",
    "]\n",
    "\n",
    "features = numeric_features + categorical_onehot_features + extra_features\n",
    "target = \"volume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "gxs_categorical_onehot_features = [\n",
    "    # ther_area\n",
    "    \"ther_area_Anti-infectives\",\n",
    "    # \"ther_area_Antineoplastic_and_immunology\",\n",
    "    \"ther_area_Cardiovascular_Metabolic\",\n",
    "    # \"ther_area_Dermatology\",\n",
    "    # \"ther_area_Endocrinology_and_Metabolic_Disease\",\n",
    "    # \"ther_area_Haematology\",\n",
    "    \"ther_area_Muscoskeletal_Rheumatology_and_Osteology\",\n",
    "    # \"ther_area_Nervous_system\",\n",
    "    # \"ther_area_Obstetrics_Gynaecology\",\n",
    "    # \"ther_area_Others\",\n",
    "    # \"ther_area_Parasitology\",\n",
    "    \"ther_area_Respiratory_and_Immuno-inflammatory\",\n",
    "    # \"ther_area_Sensory_organs\",\n",
    "    # \"ther_area_Systemic_Hormones\", \n",
    "\n",
    "    # main_package\n",
    "    \"main_package_PATCH\",\n",
    "    \"main_package_PILL\",\n",
    "\n",
    "    # biological / small molecule flags\n",
    "    \"small_molecule_False\", \"small_molecule_True\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b0621",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train_full[features]\n",
    "y = np.log1p(df_train_full[target])\n",
    "\n",
    "# Split for validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = XGBRegressor(\n",
    "    n_estimators=1200,\n",
    "    learning_rate=0.07,\n",
    "    max_depth=8,\n",
    "    subsample=0.5,\n",
    "    colsample_bytree=0.9,\n",
    "    min_child_weight=9,\n",
    "    objective=\"reg:squarederror\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42, \n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=100,\n",
    "    gamma=0,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)], \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae  = mean_absolute_error(y_val, y_pred)\n",
    "r2   = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\",  mae)\n",
    "print(\"R²:\",   r2)\n",
    "\n",
    "\n",
    "# ---- Compute metrics on validation set ----\n",
    "# m1 = compute_metric1(X_val, prediction, df_aux)\n",
    "# m2 = compute_metric2(X_val, prediction, df_aux)\n",
    "\n",
    "# print(f\"Metric 1 - Phase 1-a (local validation): {m1}\")\n",
    "# print(f\"Metric 2 - Phase 1-b (local validation): {m2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01720c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning through randomized search\n",
    "param_grid = {\n",
    "    \"n_estimators\": [800, 1000, 1200, 1500],\n",
    "    \"learning_rate\": [0.03, 0.05, 0.07, 0.1],\n",
    "    \"max_depth\": [5, 6, 7, 8],\n",
    "    \"min_child_weight\": [5, 7, 9, 11],\n",
    "    \"subsample\": [0.5, 0.6, 0.7, 0.8],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
    "\n",
    "    # Extra regularisation knobs\n",
    "    \"gamma\": [0, 0.1, 0.3],\n",
    "    \"reg_lambda\": [1, 3, 5, 10], \n",
    "    \"reg_alpha\": [0, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=30,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# search.fit(\n",
    "#     X_train, y_train,\n",
    "#     eval_set=[(X_val, y_val)], \n",
    "#     verbose=False\n",
    "# )\n",
    "# print(\"BEST:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96267589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set\n",
    "X_test = df_test_full[features]\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "df_test_full[target] = np.expm1(y_test_pred)\n",
    "df_test_full.to_csv(\"../data/processed/df_pred_unscaled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding and filling existing values between submission example and predictions\n",
    "submission_example = pd.read_csv(\"../examples_and_template/submission_example.csv\")\n",
    "pred_unscaled = pd.read_csv(\"../data/processed/df_pred_unscaled.csv\")\n",
    "\n",
    "existing_months = [\"month_Jan\", \n",
    "                   \"month_Feb\", \n",
    "                   \"month_Mar\", \n",
    "                   \"month_Apr\", \n",
    "                   \"month_May\", \n",
    "                   \"month_Jun\", \n",
    "                   \"month_Jul\", \n",
    "                   \"month_Aug\", \n",
    "                   \"month_Sep\", \n",
    "                   \"month_Oct\", \n",
    "                   \"month_Nov\", \n",
    "                   \"month_Dec\"\n",
    "                   ]\n",
    "         \n",
    "\n",
    "existing_cols = [\"hospital_rate\", \n",
    "        \"avg_vol\", \n",
    "        \"vol_norm\", \n",
    "        \"mean_generic_erosion\",  \n",
    "        \"ther_area_Anti-infectives\", \n",
    "        \"ther_area_Antineoplastic_and_immunology\", \n",
    "        \"ther_area_Cardiovascular_Metabolic\", \n",
    "        \"ther_area_Dermatology\", \n",
    "        \"ther_area_Endocrinology_and_Metabolic_Disease\", \n",
    "        \"ther_area_Haematology\", \n",
    "        \"ther_area_Muscoskeletal_Rheumatology_and_Osteology\", \n",
    "        \"ther_area_Nervous_system\", \n",
    "        \"ther_area_Obstetrics_Gynaecology\", \n",
    "        \"ther_area_Others\", \n",
    "        \"ther_area_Parasitology\", \n",
    "        \"ther_area_Respiratory_and_Immuno-inflammatory\", \n",
    "        \"ther_area_Sensory_organs\", \n",
    "        \"ther_area_Systemic_Hormones\", \n",
    "        \"main_package_CREAM\", \n",
    "        \"main_package_EYE DROP\", \n",
    "        \"main_package_INJECTION\", \n",
    "        \"main_package_Others\", \n",
    "        \"main_package_PATCH\", \n",
    "        \"main_package_PILL\", \n",
    "        \"biological_False\", \n",
    "        \"biological_True\", \n",
    "        \"small_molecule_False\", \n",
    "        \"small_molecule_True\", \n",
    "        \"bucket_1\", \n",
    "        \"bucket_2\",\n",
    "        \"lag_1\", \n",
    "        \"lag_2\", \n",
    "        \"lag_3\", \n",
    "        \"roll3_past\", \n",
    "        \"roll6_past\", \n",
    "        \"group_volume_mean\", \n",
    "        \"last_year_volume\", \n",
    "        \"bucket\"\n",
    "        ]\n",
    "\n",
    "# Ensure all required columns exist with NaN initial values\n",
    "for col in existing_months + existing_cols:\n",
    "    if col not in submission_example.columns:\n",
    "        submission_example[col] = np.nan\n",
    "\n",
    "for row in submission_example.itertuples(index=False):\n",
    "    country = row.country\n",
    "    brand_name = row.brand_name\n",
    "    months_postgx = row.months_postgx\n",
    "\n",
    "    # ----- exact match on country + brand_name + months_postgx -----\n",
    "    mask_pred_3 = (\n",
    "        (pred_unscaled[\"country\"] == country) &\n",
    "        (pred_unscaled[\"brand_name\"] == brand_name) &\n",
    "        (pred_unscaled[\"months_postgx\"] == months_postgx)\n",
    "    )\n",
    "    match_3 = pred_unscaled[mask_pred_3]\n",
    "    # same mask on submission_example\n",
    "    mask_sub_3 = (\n",
    "        (submission_example[\"country\"] == country) &\n",
    "        (submission_example[\"brand_name\"] == brand_name) &\n",
    "        (submission_example[\"months_postgx\"] == months_postgx)\n",
    "    )\n",
    "\n",
    "    if not match_3.empty:\n",
    "        first3 = match_3.iloc[0]\n",
    "\n",
    "        # fill only rows that match this exact triple\n",
    "        submission_example.loc[mask_sub_3, existing_months] = (\n",
    "            first3[existing_months].values\n",
    "        )\n",
    "        submission_example.loc[mask_sub_3, existing_cols] = (\n",
    "            first3[existing_cols].values\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # ----- fallback: match only on country + brand_name -----\n",
    "        mask_pred_2 = (\n",
    "            (pred_unscaled[\"country\"] == country) &\n",
    "            (pred_unscaled[\"brand_name\"] == brand_name)\n",
    "        )\n",
    "        match_2 = pred_unscaled[mask_pred_2]\n",
    "\n",
    "        if not match_2.empty:\n",
    "            first2 = match_2.iloc[0]\n",
    "\n",
    "            mask_sub_2 = (\n",
    "                (submission_example[\"country\"] == country) &\n",
    "                (submission_example[\"brand_name\"] == brand_name)\n",
    "            )\n",
    "\n",
    "            # only fill those rows in submission_example\n",
    "            submission_example.loc[mask_sub_2, existing_cols] = (\n",
    "                first2[existing_cols].values\n",
    "            )\n",
    "\n",
    "submission_example[\"n_gxs\"] = np.nan\n",
    "submission_example.to_csv(\"../submissions/submission_prep.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d5d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation for the “no fully-filled row” case\n",
    "default_start_month = \"month_Jun\"\n",
    "\n",
    "month_to_idx = {m: i for i, m in enumerate(existing_months)}\n",
    "n_months = len(existing_months)\n",
    "\n",
    "def infer_months_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Case A: find a row where *all* month columns are non-null\n",
    "    mask_full = group[existing_months].notna().all(axis=1)\n",
    "\n",
    "    if mask_full.any():\n",
    "        # Use the first such row as the base (should correspond to months_postgx == 0)\n",
    "        base_row = group.loc[mask_full].iloc[0]\n",
    "        # Determine which calendar month that row encodes (take the max / 1.0)\n",
    "        base_month_col = base_row[existing_months].idxmax()\n",
    "        base_idx = month_to_idx[base_month_col]\n",
    "    else:\n",
    "        # Case B: no fully-filled row -> assume months_postgx == 0 would be June\n",
    "        base_idx = month_to_idx[default_start_month]\n",
    "\n",
    "    # Compute the calendar month index for each row based on months_postgx\n",
    "    offset = group[\"months_postgx\"].astype(int)\n",
    "    month_idx = (base_idx + offset) % n_months\n",
    "\n",
    "    # Build one-hot encoding into the existing_months columns\n",
    "    for i, col in enumerate(existing_months):\n",
    "        group[col] = (month_idx == i).astype(float)\n",
    "\n",
    "    return group\n",
    "\n",
    "# Applying it to the whole dataframe\n",
    "submission_example = (\n",
    "    submission_example\n",
    "    .groupby([\"country\", \"brand_name\"], group_keys=False)\n",
    "    .apply(infer_months_for_group)\n",
    ")\n",
    "\n",
    "submission_example.to_csv(\"../submissions/submission_prep.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e3b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating prediction model for imputation of missing n_gxs values\n",
    "\n",
    "# Load datasets\n",
    "df_train_full = pd.read_csv(\"../data/processed/df_train_unscaled.csv\")\n",
    "df_test_full = pd.read_csv(\"../data/processed/df_test_unscaled.csv\")\n",
    "\n",
    "gxs_numeric_features = [\n",
    "    \"months_postgx\",\n",
    "    #\"hospital_rate\",\n",
    "    # \"mean_generic_erosion\"\n",
    "    \"lag_1\", \n",
    "    \"lag_2\", \n",
    "    \"lag_3\", \n",
    "    \"roll3_past\", \n",
    "    \"roll6_past\", \n",
    "    \"group_volume_mean\", \n",
    "    \"last_year_volume\"\n",
    "]\n",
    "\n",
    "gxs_features = gxs_numeric_features + gxs_categorical_onehot_features\n",
    "gxs_target = \"n_gxs\"\n",
    "\n",
    "gxs_X = df_train_full[gxs_features]\n",
    "gxs_y = df_train_full[gxs_target]\n",
    "\n",
    "# Split for validation sets\n",
    "gxs_X_train, gxs_X_val, gxs_y_train, gxs_y_val = train_test_split(\n",
    "    gxs_X, gxs_y,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "gxs_model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.07,\n",
    "    max_depth=8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=1.0,\n",
    "    min_child_weight=5,\n",
    "    objective=\"reg:squarederror\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42, \n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=100,\n",
    "    gamma=0.1,\n",
    "    reg_lambda=1,\n",
    "    reg_alpha=0.5\n",
    ")\n",
    "\n",
    "gxs_model.fit(\n",
    "    gxs_X_train, gxs_y_train,\n",
    "    eval_set=[(gxs_X_val, gxs_y_val)], \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "gxs_y_pred = gxs_model.predict(gxs_X_val)\n",
    "\n",
    "gxs_rmse = np.sqrt(mean_squared_error(gxs_y_val, gxs_y_pred))\n",
    "gxs_r2 = r2_score(gxs_y_val, gxs_y_pred)\n",
    "\n",
    "print(\"gxs_RMSE:\", gxs_rmse)\n",
    "print(\"gxs_R²:\", gxs_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6947d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning through randomized search\n",
    "# search.fit(\n",
    "#     gxs_X_train, gxs_y_train,\n",
    "#     eval_set=[(gxs_X_val, gxs_y_val)], \n",
    "#     verbose=False\n",
    "# )\n",
    "\n",
    "# print(\"BEST:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f94096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on submission set for n_gxs\n",
    "gxs_X_sub_prep = submission_example[gxs_features]\n",
    "gxs_y_sub_prep = gxs_model.predict(gxs_X_sub_prep)\n",
    "\n",
    "submission_example[gxs_target] = gxs_y_sub_prep\n",
    "\n",
    "submission_example.to_csv(\"../submissions/submission_prep.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f2581",
   "metadata": {},
   "outputs": [],
   "source": "# Predictions on submission set for volume\nsub_prep = pd.read_csv(\"../submissions/submission_prep.csv\")\nvol_X_sub_prep = sub_prep[features]\nvol_y_sub_prep = model.predict(vol_X_sub_prep)\n\n# Convert from log space back to original scale\nsub_prep[target] = np.expm1(vol_y_sub_prep)\n\n# Fix negative volumes by interpolation\ndf = sub_prep.copy()\n\n# Ensure groups stay in correct temporal order\ndf = df.sort_values([\"country\", \"brand_name\", \"months_postgx\"])\n\n# Convert negative volumes to NaN\ndf.loc[df[\"volume\"] < 0, \"volume\"] = np.nan\n\n# Compute global fallback start/end values from groups that have valid volumes\ngrouped = df.groupby([\"country\", \"brand_name\"])[\"volume\"]\n\nfirst_valid = grouped.first().dropna()   # first non-null per group\nlast_valid  = grouped.last().dropna()    # last non-null per group\n\nglobal_min_start = first_valid.min() if not first_valid.empty else 0.0\nglobal_min_end = last_valid.min()  if not last_valid.empty else 0.0\n\n\n# Function that repairs the volume column within each group\ndef fix_negative_volumes(group, min_start, min_end):\n    g = group.copy()\n\n    # Safety: ensure all negatives treated as NaN\n    g.loc[g[\"volume\"] < 0, \"volume\"] = np.nan\n\n    if g[\"volume\"].notna().any():\n        # Normal case: interpolate within group\n        g[\"volume\"] = g[\"volume\"].interpolate(\n            method=\"linear\",\n            limit_direction=\"both\"\n        )\n    else:\n        # Edge case: group has NO valid volumes at all, create global ramp\n        n = len(g)\n        g[\"volume\"] = np.linspace(min_start, min_end, n)\n\n    return g\n\n\n# Apply per (country, brand_name)\ndf_fixed = (\n    df.groupby([\"country\", \"brand_name\"], group_keys=False)\n      .apply(fix_negative_volumes, min_start=global_min_start, min_end=global_min_end)\n)\n\n# Insert fixed volumes back into the submission dataframe (optional)\nsub_prep[\"volume\"] = df_fixed[\"volume\"]\n\nsub_prep.to_csv(\"../submissions/submission_prep.csv\", index=False)\nrequired_columns = [\"country\", \"brand_name\", \"months_postgx\", \"volume\"]\n\n# Final submission file\nfinal_submission = sub_prep[required_columns]\nfinal_submission.to_csv(\"../submissions/final_submission.csv\", index=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cc9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance extraction\n",
    "model.get_booster().get_score(importance_type=\"gain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b2cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance extraction for n_gxs model\n",
    "gxs_model.get_booster().get_score(importance_type=\"gain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc09f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = pd.read_csv(\"../submissions/final_submission_best.csv\")\n",
    "eight = pd.read_csv(\"../submissions/final_submission_8.csv\")\n",
    "best[\"volume\"] = (best[\"volume\"] + eight[\"volume\"]) / 2.0\n",
    "best.to_csv(\"../submissions/final_submission_ensemble.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}